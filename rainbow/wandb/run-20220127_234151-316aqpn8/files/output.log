
DEBUG:urllib3.connectionpool:https://api.wandb.ai:443 "POST /graphql HTTP/1.1" 200 None
About to train 1 agents on (25,25) env.
Parameters:
max_num_cities: 2
max_rails_between_cities: 2
max_rails_in_city: 4
malfunction_rate: 0
max_duration: 50
min_duration: 20
num_episodes: 200
starting from episode: 0
max_steps: 404
eps_initial: 1
eps_decay_rate: 0.992
learning_rate: 0.03
learning_rate_decay: 0.5
done_reward: 0
deadlock_reward: -1000
batch_size: 128
attention_0.att_l                                         (1, 4, 10)
attention_0.att_r                                         (1, 4, 10)
attention_0.bias                                               (40,)
attention_0.lin_l.weight                                    (40, 12)
attention_1.att_l                                         (1, 4, 10)
attention_1.att_r                                         (1, 4, 10)
attention_1.bias                                               (40,)
attention_1.lin_l.weight                                    (40, 40)
attention_2.att_l                                         (1, 4, 10)
attention_2.att_r                                         (1, 4, 10)
attention_2.bias                                               (40,)
attention_2.lin_l.weight                                    (40, 40)
out_att.att_l                                              (1, 1, 1)
out_att.att_r                                              (1, 1, 1)
out_att.bias                                                    (1,)
out_att.lin_l.weight                                         (1, 40)
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
filename is: test_results/
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Ep: 1	 2 Agents on (25,25).	 Ep score -96.000	Avg Score: -0.238	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.99	EP ended at step: 162/404	Mean state_value: [0.8805705]	 Epoch avg_loss: None
Ep: 2	 3 Agents on (25,25).	 Ep score -283.333	Avg Score: -0.694	 Env Dones so far: 0.00%	 Done Agents in ep: 33.33%	 In deadlock 66.67%(at switch 1)
		 Not started 0	 Eps: 0.98	EP ended at step: 412/408	Mean state_value: [0.8872988]	 Epoch avg_loss: None
Ep: 3	 4 Agents on (25,25).	 Ep score -414.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)
		 Not started 0	 Eps: 0.98	EP ended at step: 416/412	Mean state_value: [0.8126491]	 Epoch avg_loss: None
Ep: 4	 5 Agents on (25,25).	 Ep score -340.800	Avg Score: -0.819	 Env Dones so far: 0.00%	 Done Agents in ep: 20.00%	 In deadlock 80.00%(at switch 0)
		 Not started 0	 Eps: 0.97	EP ended at step: 420/416	Mean state_value: [0.84164697]	 Epoch avg_loss: None
Ep: 5	 6 Agents on (25,25).	 Ep score -422.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.96	EP ended at step: 424/420	Mean state_value: [0.85055363]	 Epoch avg_loss: None
Ep: 6	 7 Agents on (25,25).	 Ep score -426.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 1)
		 Not started 0	 Eps: 0.95	EP ended at step: 428/424	Mean state_value: [0.76971185]	 Epoch avg_loss: None
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/flatland/envs/rail_generators.py:780: UserWarning: Could not set all required cities!
  warnings.warn(
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/flatland/envs/rail_generators.py:703: UserWarning: [WARNING] Changing to Grid mode to place at least 2 cities.
  warnings.warn("[WARNING] Changing to Grid mode to place at least 2 cities.")
Ep: 7	 8 Agents on (25,25).	 Ep score -254.000	Avg Score: -0.593	 Env Dones so far: 0.00%	 Done Agents in ep: 50.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.95	EP ended at step: 432/428	Mean state_value: [0.893107]	 Epoch avg_loss: None
Ep: 8	 1 Agents on (25,25).	 Ep score -22.000	Avg Score: -0.051	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.94	EP ended at step: 24/432	Mean state_value: [0.9123203]	 Epoch avg_loss: None
Traceback (most recent call last):
  File "train.py", line 457, in <module>
    main(args)
  File "train.py", line 278, in main
    next_obs, all_rewards, done, info = env.step(railenv_action_dict)
  File "/home/runnphoenix/anaconda3/lib/python3.8/site-packages/flatland/envs/rail_env.py", line 590, in step
    return self._get_observations(), self.rewards_dict, self.dones, info_dict
  File "/home/runnphoenix/anaconda3/lib/python3.8/site-packages/flatland/envs/rail_env.py", line 1070, in _get_observations
    self.obs_dict = self.obs_builder.get_many(list(range(self.get_num_agents())))
  File "/home/runnphoenix/work/flatland-rl/rainbow/graph_for_observation.py", line 555, in get_many
    obs[agent] = self.get(agent)
  File "/home/runnphoenix/work/flatland-rl/rainbow/graph_for_observation.py", line 543, in get
    unified, partitioned = self._get_graph_observation(
  File "/home/runnphoenix/work/flatland-rl/rainbow/graph_for_observation.py", line 1166, in _get_graph_observation
    track_features = self._compute_node_features(
  File "/home/runnphoenix/work/flatland-rl/rainbow/graph_for_observation.py", line 1434, in _compute_node_features
    self._log10_normalize_node_feature(track_length),
  File "/home/runnphoenix/work/flatland-rl/rainbow/graph_for_observation.py", line 1272, in _log10_normalize_node_feature
    return np.log10(feature+1)
KeyboardInterrupt