About to train 1 agents on (25,25) env.
Parameters:
max_num_cities: 2
max_rails_between_cities: 2
max_rails_in_city: 4
malfunction_rate: 0
max_duration: 50
min_duration: 20
num_episodes: 50
starting from episode: 0
max_steps: 404
eps_initial: 1
eps_decay_rate: 0.999
learning_rate: 0.005
learning_rate_decay: 1.0
done_reward: 0
deadlock_reward: -1000
batch_size: 512
attention_0.att_l                                         (1, 4, 10)
attention_0.att_r                                         (1, 4, 10)
attention_0.bias                                               (40,)
attention_0.lin_l.weight                                    (40, 12)
attention_1.att_l                                         (1, 4, 10)
attention_1.att_r                                         (1, 4, 10)
attention_1.bias                                               (40,)
attention_1.lin_l.weight                                    (40, 40)
attention_2.att_l                                         (1, 4, 10)
attention_2.att_r                                         (1, 4, 10)
attention_2.bias                                               (40,)
attention_2.lin_l.weight                                    (40, 40)
out_att.att_l                                              (1, 1, 1)
out_att.att_r                                              (1, 1, 1)
out_att.bias                                                    (1,)
out_att.lin_l.weight                                         (1, 40)
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
filename is: test_results/
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Ep: 1	 2 Agents on (25,25).	 Ep score -157.500	Avg Score: -0.390	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.80	EP ended at step: 225/404	Mean state_value: [-0.29519626]	 Epoch avg_loss: None
Ep: 2	 3 Agents on (25,25).	 Ep score -91.000	Avg Score: -0.223	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.68	EP ended at step: 156/408	Mean state_value: [-0.12398099]	 Epoch avg_loss: None
Ep: 3	 4 Agents on (25,25).	 Ep score -25.750	Avg Score: -0.062	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.66	EP ended at step: 34/412	Mean state_value: [0.0097978]	 Epoch avg_loss: None
Ep: 4	 5 Agents on (25,25).	 Ep score -261.600	Avg Score: -0.629	 Env Dones so far: 0.00%	 Done Agents in ep: 40.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.43	EP ended at step: 420/416	Mean state_value: [-0.15446545]	 Epoch avg_loss: None
Ep: 5	 6 Agents on (25,25).	 Ep score -93.667	Avg Score: -0.223	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.35	EP ended at step: 226/420	Mean state_value: [-0.07381914]	 Epoch avg_loss: None
Ep: 6	 7 Agents on (25,25).	 Ep score -77.143	Avg Score: -0.182	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.27	EP ended at step: 236/424	Mean state_value: [-0.09328865]	 Epoch avg_loss: None
Ep: 7	 8 Agents on (25,25).	 Ep score -321.250	Avg Score: -0.751	 Env Dones so far: 0.00%	 Done Agents in ep: 62.50%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.18	EP ended at step: 432/428	Mean state_value: [0.06633518]	 Epoch avg_loss: None
Ep: 8	 1 Agents on (25,25).	 Ep score -22.000	Avg Score: -0.051	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.17	EP ended at step: 24/432	Mean state_value: [0.05109394]	 Epoch avg_loss: None
Ep: 9	 2 Agents on (25,25).	 Ep score -30.000	Avg Score: -0.074	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.17	EP ended at step: 37/404	Mean state_value: [-0.1264675]	 Epoch avg_loss: None
Ep: 10	 3 Agents on (25,25).	 Ep score -15.667	Avg Score: -0.038	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.16	EP ended at step: 20/408	Mean state_value: [0.48971057]	 Epoch avg_loss: None
 DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
--------------- TESTING STARTED ------------------
Ep: 0	 1 Agents on (25,25).	 Ep score -22.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 24/404
Ep: 1	 1 Agents on (25,25).	 Ep score -14.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 16/404
Ep: 2	 1 Agents on (25,25).	 Ep score -20.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 22/404
Ep: 0	 2 Agents on (25,25).	 Ep score -28.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 36/408
Ep: 1	 2 Agents on (25,25).	 Ep score -27.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 35/408
Ep: 2	 2 Agents on (25,25).	 Ep score -23.500	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 35/408
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/flatland/envs/rail_generators.py:780: UserWarning: Could not set all required cities!
  warnings.warn(
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/flatland/envs/rail_generators.py:703: UserWarning: [WARNING] Changing to Grid mode to place at least 2 cities.
  warnings.warn("[WARNING] Changing to Grid mode to place at least 2 cities.")
Ep: 0	 3 Agents on (25,25).	 Ep score -410.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 412/412
Ep: 1	 3 Agents on (25,25).	 Ep score -19.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 34/412
Ep: 2	 3 Agents on (25,25).	 Ep score -25.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 33/412
Ep: 0	 4 Agents on (25,25).	 Ep score -318.500	 Done Agents in ep: 25.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 416/416
Ep: 1	 4 Agents on (25,25).	 Ep score -26.250	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 34/416
Ep: 2	 4 Agents on (25,25).	 Ep score -319.750	 Done Agents in ep: 25.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 416/416
Ep: 0	 5 Agents on (25,25).	 Ep score -262.000	 Done Agents in ep: 40.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 420/420
Ep: 1	 5 Agents on (25,25).	 Ep score -336.800	 Done Agents in ep: 20.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 420/420
Ep: 2	 5 Agents on (25,25).	 Ep score -257.000	 Done Agents in ep: 40.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 420/420
Ep: 0	 6 Agents on (25,25).	 Ep score -226.167	 Done Agents in ep: 50.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 424/424
Ep: 1	 6 Agents on (25,25).	 Ep score -422.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 1)	 Not started 0	EP ended at step: 424/424
Ep: 2	 6 Agents on (25,25).	 Ep score -422.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 424/424
Ep: 0	 7 Agents on (25,25).	 Ep score -200.714	 Done Agents in ep: 57.14%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 428/428
Ep: 1	 7 Agents on (25,25).	 Ep score -426.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 428/428
Ep: 2	 7 Agents on (25,25).	 Ep score -426.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 1)	 Not started 0	EP ended at step: 428/428
Ep: 0	 8 Agents on (25,25).	 Ep score -330.875	 Done Agents in ep: 25.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
Ep: 1	 8 Agents on (25,25).	 Ep score -430.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
Ep: 2	 8 Agents on (25,25).	 Ep score -329.375	 Done Agents in ep: 25.00%	 In deadlock 25.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
AGENT: Saving local and target networks
Epoch 10, testing agents on 3: Avg. done agents: 50.297619047619044% | Avg. reward: -1297.1666666666667 | Avg. normalized reward: -24.021604938271608 | Avg. agents in deadlock: 9.375%| LR: 0.005
Replay memory saved
Ep: 11	 4 Agents on (25,25).	 Ep score -45.500	Avg Score: -0.110	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.16	EP ended at step: 61/412	Mean state_value: [0.09702701]	 Epoch avg_loss: None
Ep: 12	 5 Agents on (25,25).	 Ep score -336.800	Avg Score: -0.810	 Env Dones so far: 0.00%	 Done Agents in ep: 20.00%	 In deadlock 80.00%(at switch 1)
		 Not started 0	 Eps: 0.10	EP ended at step: 420/416	Mean state_value: [-0.09816892]	 Epoch avg_loss: None
Ep: 13	 6 Agents on (25,25).	 Ep score -422.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 66.67%(at switch 0)
		 Not started 0	 Eps: 0.07	EP ended at step: 424/420	Mean state_value: [-0.07912163]	 Epoch avg_loss: None
Ep: 14	 7 Agents on (25,25).	 Ep score -426.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)
		 Not started 0	 Eps: 0.04	EP ended at step: 428/424	Mean state_value: [-0.09318335]	 Epoch avg_loss: None
Ep: 15	 8 Agents on (25,25).	 Ep score -263.375	Avg Score: -0.615	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.03	EP ended at step: 339/428	Mean state_value: [0.7149166]	 Epoch avg_loss: 209.12682461738586
Ep: 16	 1 Agents on (25,25).	 Ep score -14.000	Avg Score: -0.032	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.03	EP ended at step: 16/432	Mean state_value: [2.1304698]	 Epoch avg_loss: None
Ep: 17	 2 Agents on (25,25).	 Ep score -24.000	Avg Score: -0.059	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.03	EP ended at step: 31/404	Mean state_value: [1.9725674]	 Epoch avg_loss: 1037.491943359375
Ep: 18	 3 Agents on (25,25).	 Ep score -278.000	Avg Score: -0.681	 Env Dones so far: 0.00%	 Done Agents in ep: 33.33%	 In deadlock 66.67%(at switch 2)
		 Not started 0	 Eps: 0.02	EP ended at step: 412/408	Mean state_value: [2.2733264]	 Epoch avg_loss: None
Ep: 19	 4 Agents on (25,25).	 Ep score -414.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 416/412	Mean state_value: [2.253035]	 Epoch avg_loss: 878.0137939453125
Ep: 20	 5 Agents on (25,25).	 Ep score -257.000	Avg Score: -0.618	 Env Dones so far: 0.00%	 Done Agents in ep: 40.00%	 In deadlock 40.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 420/416	Mean state_value: [2.521945]	 Epoch avg_loss: 176.07573127746582
 DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
--------------- TESTING STARTED ------------------
Ep: 0	 1 Agents on (25,25).	 Ep score -22.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 24/404
Ep: 1	 1 Agents on (25,25).	 Ep score -14.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 16/404
Ep: 2	 1 Agents on (25,25).	 Ep score -30.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 32/404
Ep: 0	 2 Agents on (25,25).	 Ep score -28.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 36/408
Ep: 1	 2 Agents on (25,25).	 Ep score -27.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 35/408
Ep: 2	 2 Agents on (25,25).	 Ep score -34.500	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 57/408
Ep: 0	 3 Agents on (25,25).	 Ep score -26.333	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 36/412
Ep: 1	 3 Agents on (25,25).	 Ep score -17.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 20/412
Ep: 2	 3 Agents on (25,25).	 Ep score -31.667	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 35/412
Ep: 0	 4 Agents on (25,25).	 Ep score -27.750	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 36/416
Ep: 1	 4 Agents on (25,25).	 Ep score -25.750	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 33/416
Ep: 2	 4 Agents on (25,25).	 Ep score -21.750	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 32/416
Ep: 0	 5 Agents on (25,25).	 Ep score -27.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 36/420
Ep: 1	 5 Agents on (25,25).	 Ep score -257.000	 Done Agents in ep: 40.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 420/420
Ep: 2	 5 Agents on (25,25).	 Ep score -260.400	 Done Agents in ep: 40.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 420/420
Ep: 0	 6 Agents on (25,25).	 Ep score -28.333	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 37/424
Ep: 1	 6 Agents on (25,25).	 Ep score -357.000	 Done Agents in ep: 16.67%	 In deadlock 83.33%(at switch 0)	 Not started 0	EP ended at step: 424/424
Ep: 2	 6 Agents on (25,25).	 Ep score -21.500	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 31/424
Ep: 0	 7 Agents on (25,25).	 Ep score -39.143	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 106/428
Ep: 1	 7 Agents on (25,25).	 Ep score -370.429	 Done Agents in ep: 14.29%	 In deadlock 71.43%(at switch 2)	 Not started 0	EP ended at step: 428/428
Ep: 2	 7 Agents on (25,25).	 Ep score -426.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 428/428
Ep: 0	 8 Agents on (25,25).	 Ep score -179.500	 Done Agents in ep: 62.50%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
Ep: 1	 8 Agents on (25,25).	 Ep score -26.625	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 34/432
Ep: 2	 8 Agents on (25,25).	 Ep score -380.250	 Done Agents in ep: 12.50%	 In deadlock 50.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
AGENT: Saving local and target networks
Epoch 20, testing agents on 3: Avg. done agents: 78.58134920634922% | Avg. reward: -690.9166666666666 | Avg. normalized reward: -12.794753086419753 | Avg. agents in deadlock: 12.698412698412698%| LR: 0.005
Replay memory saved
Ep: 21	 6 Agents on (25,25).	 Ep score -422.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 424/420	Mean state_value: [3.0313969]	 Epoch avg_loss: 62.323326110839844
Ep: 22	 7 Agents on (25,25).	 Ep score -369.857	Avg Score: -0.872	 Env Dones so far: 0.00%	 Done Agents in ep: 14.29%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 428/424	Mean state_value: [3.5542228]	 Epoch avg_loss: 155.7128510908647
Ep: 23	 8 Agents on (25,25).	 Ep score -430.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 62.50%(at switch 2)
		 Not started 0	 Eps: 0.01	EP ended at step: 432/428	Mean state_value: [2.7785106]	 Epoch avg_loss: 221.26451110839844
Ep: 24	 1 Agents on (25,25).	 Ep score -30.000	Avg Score: -0.069	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 32/432	Mean state_value: [2.79848]	 Epoch avg_loss: 148.43292236328125
Ep: 25	 2 Agents on (25,25).	 Ep score -184.000	Avg Score: -0.455	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 357/404	Mean state_value: [2.6022215]	 Epoch avg_loss: 148.5745849609375
Ep: 26	 3 Agents on (25,25).	 Ep score -28.000	Avg Score: -0.069	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 35/408	Mean state_value: [2.8764093]	 Epoch avg_loss: 196.28985595703125
Ep: 27	 4 Agents on (25,25).	 Ep score -33.250	Avg Score: -0.081	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 37/412	Mean state_value: [2.7377536]	 Epoch avg_loss: 124.69949340820312
Ep: 28	 5 Agents on (25,25).	 Ep score -35.400	Avg Score: -0.085	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 76/416	Mean state_value: [2.9684708]	 Epoch avg_loss: 148.83495330810547
Ep: 29	 6 Agents on (25,25).	 Ep score -422.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 66.67%(at switch 1)
		 Not started 0	 Eps: 0.01	EP ended at step: 424/420	Mean state_value: [2.7326744]	 Epoch avg_loss: 111.95481872558594
Ep: 30	 7 Agents on (25,25).	 Ep score -255.429	Avg Score: -0.602	 Env Dones so far: 0.00%	 Done Agents in ep: 42.86%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 428/424	Mean state_value: [2.687408]	 Epoch avg_loss: 196.5542460123698
 DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
--------------- TESTING STARTED ------------------
Ep: 0	 1 Agents on (25,25).	 Ep score -22.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 24/404
Ep: 1	 1 Agents on (25,25).	 Ep score -402.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 404/404
Ep: 2	 1 Agents on (25,25).	 Ep score -402.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 404/404
Ep: 0	 2 Agents on (25,25).	 Ep score -28.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 36/408
Ep: 1	 2 Agents on (25,25).	 Ep score -27.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 35/408
Ep: 2	 2 Agents on (25,25).	 Ep score -34.500	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 57/408
Ep: 0	 3 Agents on (25,25).	 Ep score -26.333	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 36/412
Ep: 1	 3 Agents on (25,25).	 Ep score -17.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 20/412
Ep: 2	 3 Agents on (25,25).	 Ep score -31.667	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 35/412
Ep: 0	 4 Agents on (25,25).	 Ep score -27.750	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 36/416
Ep: 1	 4 Agents on (25,25).	 Ep score -25.750	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 33/416
Ep: 2	 4 Agents on (25,25).	 Ep score -21.750	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 32/416
Ep: 0	 5 Agents on (25,25).	 Ep score -27.000	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 36/420
Ep: 1	 5 Agents on (25,25).	 Ep score -257.000	 Done Agents in ep: 40.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 420/420
Ep: 2	 5 Agents on (25,25).	 Ep score -260.400	 Done Agents in ep: 40.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 420/420
Ep: 0	 6 Agents on (25,25).	 Ep score -28.333	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 37/424
Ep: 1	 6 Agents on (25,25).	 Ep score -357.000	 Done Agents in ep: 16.67%	 In deadlock 83.33%(at switch 0)	 Not started 0	EP ended at step: 424/424
Ep: 2	 6 Agents on (25,25).	 Ep score -21.500	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 31/424
Ep: 0	 7 Agents on (25,25).	 Ep score -39.143	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 106/428
Ep: 1	 7 Agents on (25,25).	 Ep score -370.429	 Done Agents in ep: 14.29%	 In deadlock 71.43%(at switch 2)	 Not started 0	EP ended at step: 428/428
Ep: 2	 7 Agents on (25,25).	 Ep score -426.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 428/428
Ep: 0	 8 Agents on (25,25).	 Ep score -179.500	 Done Agents in ep: 62.50%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
Ep: 1	 8 Agents on (25,25).	 Ep score -26.625	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 34/432
Ep: 2	 8 Agents on (25,25).	 Ep score -430.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
Epoch 30, testing agents on 3: Avg. done agents: 69.72718253968253% | Avg. reward: -739.1666666666666 | Avg. normalized reward: -13.68827160493827 | Avg. agents in deadlock: 14.781746031746032%| LR: 0.005
Replay memory saved
Ep: 31	 8 Agents on (25,25).	 Ep score -222.750	Avg Score: -0.520	 Env Dones so far: 0.00%	 Done Agents in ep: 50.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 432/428	Mean state_value: [3.6417706]	 Epoch avg_loss: 158.1017770767212
Ep: 32	 1 Agents on (25,25).	 Ep score -22.000	Avg Score: -0.051	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 24/432	Mean state_value: [3.3617692]	 Epoch avg_loss: None
Ep: 33	 2 Agents on (25,25).	 Ep score -23.000	Avg Score: -0.057	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 34/404	Mean state_value: [3.0305943]	 Epoch avg_loss: 80.46783447265625
Ep: 34	 3 Agents on (25,25).	 Ep score -19.000	Avg Score: -0.047	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 34/408	Mean state_value: [2.905671]	 Epoch avg_loss: 44.69039535522461
/home/runnphoenix/work/flatland-rl/rainbow/prioritized_memory.py:43: RuntimeWarning: divide by zero encountered in power
  is_weight = np.power(self.tree.n_entries * sampling_probabilities, -self.beta)
/home/runnphoenix/work/flatland-rl/rainbow/prioritized_memory.py:44: RuntimeWarning: invalid value encountered in true_divide
  is_weight /= is_weight.max()
/home/runnphoenix/work/flatland-rl/rainbow/dueling_double_dqn.py:268: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  batch = np.array(batch).transpose()
Traceback (most recent call last):
  File "train.py", line 389, in <module>
    main(args)
  File "train.py", line 223, in main
    ep_controller.save_experience_and_train(a, railenv_action_dict[a], all_rewards[a], next_obs[a], done[a], step, args, ep)
  File "/home/runnphoenix/work/flatland-rl/rainbow/graph_for_observation.py", line 283, in save_experience_and_train
    step_loss = self.rl_agent.step(self.agent_path_obs_buffer[a], self.acc_rewards[a], next_obs, self.agent_done_removed[a], self.agents_in_deadlock[a], ep=ep)
  File "/home/runnphoenix/work/flatland-rl/rainbow/dueling_double_dqn.py", line 161, in step
    return self.learn(GAMMA, ep)
  File "/home/runnphoenix/work/flatland-rl/rainbow/dueling_double_dqn.py", line 275, in learn
    Q_expected = self.compute_Q_values(states, "local")
  File "/home/runnphoenix/work/flatland-rl/rainbow/dueling_double_dqn.py", line 371, in compute_Q_values
    data = Data(x=state["node_features"],
IndexError: invalid index to scalar variable.