About to train 1 agents on (25,25) env.
Parameters:
max_num_cities: 2
max_rails_between_cities: 2
max_rails_in_city: 4
malfunction_rate: 0
max_duration: 50
min_duration: 20
num_episodes: 200
starting from episode: 0
max_steps: 404
eps_initial: 1
eps_decay_rate: 0.999
learning_rate: 0.005
learning_rate_decay: 1.0
done_reward: 0
deadlock_reward: -1000
batch_size: 512
attention_0.att_l                                         (1, 4, 10)
attention_0.att_r                                         (1, 4, 10)
attention_0.bias                                               (40,)
attention_0.lin_l.weight                                    (40, 14)
attention_1.att_l                                         (1, 4, 10)
attention_1.att_r                                         (1, 4, 10)
attention_1.bias                                               (40,)
attention_1.lin_l.weight                                    (40, 40)
attention_2.att_l                                         (1, 4, 10)
attention_2.att_r                                         (1, 4, 10)
attention_2.bias                                               (40,)
attention_2.lin_l.weight                                    (40, 40)
out_att.att_l                                              (1, 1, 1)
out_att.att_r                                              (1, 1, 1)
out_att.bias                                                    (1,)
out_att.lin_l.weight                                         (1, 40)
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
filename is: test_results/
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Ep: 1	 2 Agents on (25,25).	 Ep score -93.000	Avg Score: -0.230	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.86	EP ended at step: 156/404	Mean state_value: [1.6101458]	 Epoch avg_loss: None
Ep: 2	 3 Agents on (25,25).	 Ep score -23.667	Avg Score: -0.058	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.83	EP ended at step: 32/408	Mean state_value: [1.5312322]	 Epoch avg_loss: None
Ep: 3	 4 Agents on (25,25).	 Ep score -59.250	Avg Score: -0.144	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.70	EP ended at step: 168/412	Mean state_value: [1.6694602]	 Epoch avg_loss: None
Ep: 4	 5 Agents on (25,25).	 Ep score -51.800	Avg Score: -0.125	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.60	EP ended at step: 158/416	Mean state_value: [1.5961196]	 Epoch avg_loss: None
Ep: 5	 6 Agents on (25,25).	 Ep score -355.333	Avg Score: -0.846	 Env Dones so far: 0.00%	 Done Agents in ep: 16.67%	 In deadlock 83.33%(at switch 0)
		 Not started 0	 Eps: 0.39	EP ended at step: 424/420	Mean state_value: [1.4721358]	 Epoch avg_loss: None
Ep: 6	 7 Agents on (25,25).	 Ep score -426.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 57.14%(at switch 0)
		 Not started 0	 Eps: 0.26	EP ended at step: 428/424	Mean state_value: [1.8419029]	 Epoch avg_loss: None
Ep: 7	 8 Agents on (25,25).	 Ep score -380.250	Avg Score: -0.888	 Env Dones so far: 0.00%	 Done Agents in ep: 12.50%	 In deadlock 87.50%(at switch 0)
		 Not started 0	 Eps: 0.17	EP ended at step: 432/428	Mean state_value: [2.0020247]	 Epoch avg_loss: None
Ep: 8	 1 Agents on (25,25).	 Ep score -402.000	Avg Score: -0.931	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.11	EP ended at step: 404/432	Mean state_value: [1.6406306]	 Epoch avg_loss: None
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/flatland/envs/rail_generators.py:780: UserWarning: Could not set all required cities!
  warnings.warn(
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/flatland/envs/rail_generators.py:703: UserWarning: [WARNING] Changing to Grid mode to place at least 2 cities.
  warnings.warn("[WARNING] Changing to Grid mode to place at least 2 cities.")
Ep: 9	 2 Agents on (25,25).	 Ep score -406.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.07	EP ended at step: 408/404	Mean state_value: [1.6262822]	 Epoch avg_loss: None
Ep: 10	 3 Agents on (25,25).	 Ep score -410.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.05	EP ended at step: 412/408	Mean state_value: [2.3405688]	 Epoch avg_loss: None
 DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
--------------- TESTING STARTED ------------------
Ep: 0	 1 Agents on (25,25).	 Ep score -402.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 404/404
Ep: 1	 1 Agents on (25,25).	 Ep score -402.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 404/404
Ep: 2	 1 Agents on (25,25).	 Ep score -402.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 404/404
Ep: 0	 2 Agents on (25,25).	 Ep score -406.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 408/408
Ep: 1	 2 Agents on (25,25).	 Ep score -406.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 408/408
Ep: 2	 2 Agents on (25,25).	 Ep score -406.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 408/408
Ep: 0	 3 Agents on (25,25).	 Ep score -150.333	 Done Agents in ep: 66.67%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 412/412
Ep: 1	 3 Agents on (25,25).	 Ep score -410.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 412/412
Ep: 2	 3 Agents on (25,25).	 Ep score -410.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 412/412
Ep: 0	 4 Agents on (25,25).	 Ep score -217.250	 Done Agents in ep: 50.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 416/416
Traceback (most recent call last):
  File "train.py", line 435, in <module>
    main(args)
  File "train.py", line 289, in main
    avg_done_agents, avg_reward, avg_norm_reward, avg_deadlock_agents, test_actions = test(
  File "/home/runnphoenix/work/flatland-rl/rainbow/test.py", line 196, in test
    actions, mean_fields, q_values = infer_acts(states, actions)
  File "/home/runnphoenix/work/flatland-rl/rainbow/test.py", line 191, in infer_acts
    q_action = dqn_agent.act(state)
  File "/home/runnphoenix/work/flatland-rl/rainbow/dueling_double_dqn.py", line 185, in act
    out_value = self.qnetwork_value_local(batch.x, batch.edge_index)
  File "/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/runnphoenix/work/flatland-rl/rainbow/model.py", line 175, in forward
    x = self.attentions[l](x, adj)
  File "/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 727, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch_geometric/nn/conv/gat_conv.py", line 147, in forward
    edge_index, _ = remove_self_loops(edge_index)
  File "/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch_geometric/utils/loop.py", line 33, in remove_self_loops
    edge_index = edge_index[:, mask]
KeyboardInterrupt