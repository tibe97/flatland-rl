About to train 1 agents on (25,25) env.
Parameters:
max_num_cities: 2
max_rails_between_cities: 2
max_rails_in_city: 4
malfunction_rate: 0
max_duration: 50
min_duration: 20
num_episodes: 1000
starting from episode: 0
max_steps: 404
eps_initial: 1
eps_decay_rate: 0.999
learning_rate: 0.005
learning_rate_decay: 1.0
done_reward: 0
deadlock_reward: -1000
attention_0.att_l                                         (1, 4, 10)
attention_0.att_r                                         (1, 4, 10)
attention_0.bias                                               (40,)
attention_0.lin_l.weight                                    (40, 12)
attention_1.att_l                                         (1, 4, 10)
attention_1.att_r                                         (1, 4, 10)
attention_1.bias                                               (40,)
attention_1.lin_l.weight                                    (40, 40)
attention_2.att_l                                         (1, 4, 10)
attention_2.att_r                                         (1, 4, 10)
attention_2.bias                                               (40,)
attention_2.lin_l.weight                                    (40, 40)
out_att.att_l                                              (1, 1, 1)
out_att.att_r                                              (1, 1, 1)
out_att.bias                                                    (1,)
out_att.lin_l.weight                                         (1, 40)
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
filename is: test_results/
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Ep: 1	 2 Agents on (25,25).	 Ep score -157.500	Avg Score: -0.390	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.80	EP ended at step: 225/404	Mean state_value: [0.9589888]	 Epoch avg_loss: None
Ep: 2	 3 Agents on (25,25).	 Ep score -280.333	Avg Score: -0.687	 Env Dones so far: 0.00%	 Done Agents in ep: 33.33%	 In deadlock 66.67%(at switch 0)
		 Not started 0	 Eps: 0.53	EP ended at step: 412/408	Mean state_value: [0.8446731]	 Epoch avg_loss: None
Ep: 3	 4 Agents on (25,25).	 Ep score -414.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)
		 Not started 0	 Eps: 0.35	EP ended at step: 416/412	Mean state_value: [0.89810365]	 Epoch avg_loss: None
Ep: 4	 5 Agents on (25,25).	 Ep score -418.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 40.00%(at switch 0)
		 Not started 0	 Eps: 0.23	EP ended at step: 420/416	Mean state_value: [0.8443994]	 Epoch avg_loss: None
Ep: 5	 6 Agents on (25,25).	 Ep score -422.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)
		 Not started 0	 Eps: 0.15	EP ended at step: 424/420	Mean state_value: [0.87803686]	 Epoch avg_loss: None
Ep: 6	 7 Agents on (25,25).	 Ep score -426.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)
		 Not started 0	 Eps: 0.10	EP ended at step: 428/424	Mean state_value: [0.874418]	 Epoch avg_loss: None
Ep: 7	 8 Agents on (25,25).	 Ep score -430.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)
		 Not started 0	 Eps: 0.06	EP ended at step: 432/428	Mean state_value: [0.87337744]	 Epoch avg_loss: None
Ep: 8	 1 Agents on (25,25).	 Ep score -402.000	Avg Score: -0.931	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.04	EP ended at step: 404/432	Mean state_value: [1.0455973]	 Epoch avg_loss: None
Ep: 9	 2 Agents on (25,25).	 Ep score -406.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.03	EP ended at step: 408/404	Mean state_value: [0.9782697]	 Epoch avg_loss: None
Ep: 10	 3 Agents on (25,25).	 Ep score -410.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)
		 Not started 0	 Eps: 0.02	EP ended at step: 412/408	Mean state_value: [1.0206972]	 Epoch avg_loss: None
 DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
--------------- TESTING STARTED ------------------
Ep: 0	 1 Agents on (25,25).	 Ep score -402.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 404/404
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/flatland/envs/rail_generators.py:780: UserWarning: Could not set all required cities!
  warnings.warn(
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/flatland/envs/rail_generators.py:703: UserWarning: [WARNING] Changing to Grid mode to place at least 2 cities.
  warnings.warn("[WARNING] Changing to Grid mode to place at least 2 cities.")
Ep: 1	 1 Agents on (25,25).	 Ep score -402.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 404/404
Ep: 2	 1 Agents on (25,25).	 Ep score -402.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 404/404
Ep: 0	 2 Agents on (25,25).	 Ep score -406.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 408/408
Ep: 1	 2 Agents on (25,25).	 Ep score -406.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 408/408
Ep: 2	 2 Agents on (25,25).	 Ep score -406.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 408/408
Ep: 0	 3 Agents on (25,25).	 Ep score -410.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 412/412
Ep: 1	 3 Agents on (25,25).	 Ep score -410.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 412/412
Ep: 2	 3 Agents on (25,25).	 Ep score -410.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 1)	 Not started 0	EP ended at step: 412/412
Ep: 0	 4 Agents on (25,25).	 Ep score -414.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 416/416
Ep: 1	 4 Agents on (25,25).	 Ep score -414.000	 Done Agents in ep: 0.00%	 In deadlock 50.00%(at switch 2)	 Not started 0	EP ended at step: 416/416
Ep: 2	 4 Agents on (25,25).	 Ep score -414.000	 Done Agents in ep: 0.00%	 In deadlock 75.00%(at switch 0)	 Not started 0	EP ended at step: 416/416
Ep: 0	 5 Agents on (25,25).	 Ep score -418.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 420/420
Ep: 1	 5 Agents on (25,25).	 Ep score -418.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 2)	 Not started 0	EP ended at step: 420/420
Ep: 2	 5 Agents on (25,25).	 Ep score -418.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 420/420
Ep: 0	 6 Agents on (25,25).	 Ep score -422.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 424/424
Ep: 1	 6 Agents on (25,25).	 Ep score -422.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 424/424
Ep: 2	 6 Agents on (25,25).	 Ep score -291.167	 Done Agents in ep: 33.33%	 In deadlock 66.67%(at switch 0)	 Not started 0	EP ended at step: 424/424
Ep: 0	 7 Agents on (25,25).	 Ep score -426.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 428/428
Ep: 1	 7 Agents on (25,25).	 Ep score -253.143	 Done Agents in ep: 42.86%	 In deadlock 57.14%(at switch 0)	 Not started 0	EP ended at step: 428/428
Ep: 2	 7 Agents on (25,25).	 Ep score -426.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 428/428
Ep: 0	 8 Agents on (25,25).	 Ep score -430.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
Ep: 1	 8 Agents on (25,25).	 Ep score -144.375	 Done Agents in ep: 75.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
Ep: 2	 8 Agents on (25,25).	 Ep score -430.000	 Done Agents in ep: 0.00%	 In deadlock 50.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
AGENT: Saving local and target networks
Epoch 10, testing agents on 3: Avg. done agents: 6.299603174603174% | Avg. reward: -1719.1666666666667 | Avg. normalized reward: -31.83641975308642 | Avg. agents in deadlock: 58.28373015873015%| LR: 0.005
Replay memory saved
Ep: 11	 4 Agents on (25,25).	 Ep score -414.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 50.00%(at switch 2)
		 Not started 0	 Eps: 0.01	EP ended at step: 416/412	Mean state_value: [1.3011577]	 Epoch avg_loss: None
Ep: 12	 5 Agents on (25,25).	 Ep score -418.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 2)
		 Not started 0	 Eps: 0.01	EP ended at step: 420/416	Mean state_value: [0.8624809]	 Epoch avg_loss: None
Ep: 13	 6 Agents on (25,25).	 Ep score -422.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 424/420	Mean state_value: [1.4386503]	 Epoch avg_loss: None
Ep: 14	 7 Agents on (25,25).	 Ep score -253.143	Avg Score: -0.597	 Env Dones so far: 0.00%	 Done Agents in ep: 42.86%	 In deadlock 57.14%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 428/424	Mean state_value: [0.99667424]	 Epoch avg_loss: None
Ep: 15	 8 Agents on (25,25).	 Ep score -144.375	Avg Score: -0.337	 Env Dones so far: 0.00%	 Done Agents in ep: 75.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 432/428	Mean state_value: [0.8964954]	 Epoch avg_loss: None
Ep: 16	 1 Agents on (25,25).	 Ep score -402.000	Avg Score: -0.931	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 404/432	Mean state_value: [0.8968863]	 Epoch avg_loss: None
Ep: 17	 2 Agents on (25,25).	 Ep score -406.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 408/404	Mean state_value: [1.0491021]	 Epoch avg_loss: None
Ep: 18	 3 Agents on (25,25).	 Ep score -410.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 1)
		 Not started 0	 Eps: 0.01	EP ended at step: 412/408	Mean state_value: [0.9952617]	 Epoch avg_loss: None
Ep: 19	 4 Agents on (25,25).	 Ep score -414.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 75.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 416/412	Mean state_value: [1.1703067]	 Epoch avg_loss: None
Ep: 20	 5 Agents on (25,25).	 Ep score -418.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 420/416	Mean state_value: [1.6666085]	 Epoch avg_loss: None
 DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
--------------- TESTING STARTED ------------------
Ep: 0	 1 Agents on (25,25).	 Ep score -402.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 404/404
Ep: 1	 1 Agents on (25,25).	 Ep score -402.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 404/404
Ep: 2	 1 Agents on (25,25).	 Ep score -402.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 404/404
Ep: 0	 2 Agents on (25,25).	 Ep score -406.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 408/408
Ep: 1	 2 Agents on (25,25).	 Ep score -406.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 408/408
Ep: 2	 2 Agents on (25,25).	 Ep score -406.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 408/408
Ep: 0	 3 Agents on (25,25).	 Ep score -410.000	 Done Agents in ep: 0.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 412/412
Ep: 1	 3 Agents on (25,25).	 Ep score -410.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 412/412
Ep: 2	 3 Agents on (25,25).	 Ep score -410.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 1)	 Not started 0	EP ended at step: 412/412
Ep: 0	 4 Agents on (25,25).	 Ep score -414.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 416/416
Ep: 1	 4 Agents on (25,25).	 Ep score -414.000	 Done Agents in ep: 0.00%	 In deadlock 50.00%(at switch 2)	 Not started 0	EP ended at step: 416/416
Ep: 2	 4 Agents on (25,25).	 Ep score -414.000	 Done Agents in ep: 0.00%	 In deadlock 75.00%(at switch 0)	 Not started 0	EP ended at step: 416/416
Ep: 0	 5 Agents on (25,25).	 Ep score -418.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 420/420
Ep: 1	 5 Agents on (25,25).	 Ep score -418.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 2)	 Not started 0	EP ended at step: 420/420
Ep: 2	 5 Agents on (25,25).	 Ep score -418.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 420/420
Ep: 0	 6 Agents on (25,25).	 Ep score -422.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 424/424
Ep: 1	 6 Agents on (25,25).	 Ep score -422.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 424/424
Ep: 2	 6 Agents on (25,25).	 Ep score -291.167	 Done Agents in ep: 33.33%	 In deadlock 66.67%(at switch 0)	 Not started 0	EP ended at step: 424/424
Ep: 0	 7 Agents on (25,25).	 Ep score -426.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 428/428
Ep: 1	 7 Agents on (25,25).	 Ep score -253.143	 Done Agents in ep: 42.86%	 In deadlock 57.14%(at switch 0)	 Not started 0	EP ended at step: 428/428
Ep: 2	 7 Agents on (25,25).	 Ep score -426.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 428/428
Ep: 0	 8 Agents on (25,25).	 Ep score -430.000	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
Ep: 1	 8 Agents on (25,25).	 Ep score -144.375	 Done Agents in ep: 75.00%	 In deadlock 0.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
Ep: 2	 8 Agents on (25,25).	 Ep score -430.000	 Done Agents in ep: 0.00%	 In deadlock 50.00%(at switch 0)	 Not started 0	EP ended at step: 432/432
Epoch 20, testing agents on 3: Avg. done agents: 6.299603174603174% | Avg. reward: -1719.1666666666667 | Avg. normalized reward: -31.83641975308642 | Avg. agents in deadlock: 58.28373015873015%| LR: 0.005
Replay memory saved
Ep: 21	 6 Agents on (25,25).	 Ep score -291.167	Avg Score: -0.693	 Env Dones so far: 0.00%	 Done Agents in ep: 33.33%	 In deadlock 66.67%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 424/420	Mean state_value: [1.4850112]	 Epoch avg_loss: None
Ep: 22	 7 Agents on (25,25).	 Ep score -426.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)
		 Not started 0	 Eps: 0.01	EP ended at step: 428/424	Mean state_value: [1.172463]	 Epoch avg_loss: None
Traceback (most recent call last):
  File "train.py", line 386, in <module>
    main(args)
  File "train.py", line 220, in main
    ep_controller.save_experience_and_train(a, railenv_action_dict[a], all_rewards[a], next_obs[a], done[a], step, args, ep)
  File "/home/runnphoenix/work/flatland-rl/rainbow/graph_for_observation.py", line 283, in save_experience_and_train
    step_loss = self.rl_agent.step(self.agent_path_obs_buffer[a], self.acc_rewards[a], next_obs, self.agent_done_removed[a], self.agents_in_deadlock[a], ep=ep)
  File "/home/runnphoenix/work/flatland-rl/rainbow/dueling_double_dqn.py", line 161, in step
    return self.learn(GAMMA, ep)
  File "/home/runnphoenix/work/flatland-rl/rainbow/dueling_double_dqn.py", line 292, in learn
    Q_targets_next = torch.stack(Q_targets_next).squeeze()
RuntimeError: All input tensors must be on the same device. Received cpu and cuda:0
 xxxxxxxxxxxx
[tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([11.4189], device='cuda:0', grad_fn=<UnbindBackward>), tensor([8.1446], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([-2.0473], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([37.1913], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([-3.0004], dtype=torch.float64), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([14.6428], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([2.7397e-08], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([6.2030], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], dtype=torch.float64), tensor([35.4115], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([27.6756], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.1230], device='cuda:0', grad_fn=<UnbindBackward>), tensor([2.4483e-07], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.3709], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([1.9033], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([-3.0004], dtype=torch.float64), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([-3.0004], dtype=torch.float64), tensor([1.3636e-22], device='cuda:0', grad_fn=<UnbindBackward>), tensor([3.1896e-06], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-2.1812], device='cuda:0', grad_fn=<UnbindBackward>), tensor([27.0010], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([-3.0004], dtype=torch.float64), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([-3.0004], dtype=torch.float64), tensor([22.0668], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([4.7935], device='cuda:0', grad_fn=<UnbindBackward>), tensor([4.0671], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([1.1246e-16], device='cuda:0', grad_fn=<UnbindBackward>), tensor([4.5637e-05], device='cuda:0', grad_fn=<UnbindBackward>), tensor([6.6865e-14], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([22.5276], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>), tensor([0.], device='cuda:0', grad_fn=<UnbindBackward>)]