About to train 1 agents on (25,25) env.
Parameters:
max_num_cities: 2
max_rails_between_cities: 2
max_rails_in_city: 4
malfunction_rate: 0
max_duration: 50
min_duration: 20
num_episodes: 200
starting from episode: 0
max_steps: 404
eps_initial: 1
eps_decay_rate: 0.992
learning_rate: 0.03
learning_rate_decay: 0.1
done_reward: 0
deadlock_reward: -1000
batch_size: 128
attention_0.att_l                                         (1, 4, 10)
attention_0.att_r                                         (1, 4, 10)
attention_0.bias                                               (40,)
attention_0.lin_l.weight                                    (40, 16)
attention_1.att_l                                         (1, 4, 10)
attention_1.att_r                                         (1, 4, 10)
attention_1.bias                                               (40,)
attention_1.lin_l.weight                                    (40, 40)
attention_2.att_l                                         (1, 4, 10)
attention_2.att_r                                         (1, 4, 10)
attention_2.bias                                               (40,)
attention_2.lin_l.weight                                    (40, 40)
out_att.att_l                                              (1, 1, 1)
out_att.att_r                                              (1, 1, 1)
out_att.bias                                                    (1,)
out_att.lin_l.weight                                         (1, 40)
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
DEPRECATED - RailEnv arg: malfunction_and_process_data - use malfunction_generator
filename is: test_results/
/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch/optim/lr_scheduler.py:131: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate
  warnings.warn("Detected call of `lr_scheduler.step()` before `optimizer.step()`. "
Ep: 1	 2 Agents on (25,25).	 Ep score -96.000	Avg Score: -0.238	 Env Dones so far: 100.00%	 Done Agents in ep: 100.00%	 In deadlock 0.00%(at switch 0)
		 Not started 0	 Eps: 0.99	EP ended at step: 162/404	Mean state_value: [0.58148]	 Epoch avg_loss: None
Ep: 2	 3 Agents on (25,25).	 Ep score -283.333	Avg Score: -0.694	 Env Dones so far: 0.00%	 Done Agents in ep: 33.33%	 In deadlock 66.67%(at switch 1)
		 Not started 0	 Eps: 0.98	EP ended at step: 412/408	Mean state_value: [0.5406756]	 Epoch avg_loss: None
Ep: 3	 4 Agents on (25,25).	 Ep score -414.000	Avg Score: -1.005	 Env Dones so far: 0.00%	 Done Agents in ep: 0.00%	 In deadlock 100.00%(at switch 0)
		 Not started 0	 Eps: 0.98	EP ended at step: 416/412	Mean state_value: [0.58648074]	 Epoch avg_loss: None
Ep: 4	 5 Agents on (25,25).	 Ep score -340.800	Avg Score: -0.819	 Env Dones so far: 0.00%	 Done Agents in ep: 20.00%	 In deadlock 80.00%(at switch 0)
		 Not started 0	 Eps: 0.97	EP ended at step: 420/416	Mean state_value: [0.5475777]	 Epoch avg_loss: None
Traceback (most recent call last):
  File "train.py", line 465, in <module>
    main(args)
  File "train.py", line 299, in main
    ep_controller.save_experience_and_train(a, railenv_action_dict[a], all_rewards[a], next_obs[a], done[a], step, args, ep, mean_fields[a], next_q_values[a])
  File "/home/runnphoenix/work/flatland-rl/rainbow/graph_for_observation.py", line 251, in save_experience_and_train
    step_loss = self.rl_agent.step(
  File "/home/runnphoenix/work/flatland-rl/rainbow/dueling_double_dqn.py", line 144, in step
    self._append_sample(state, reward, next_state, mean_field, next_q_value, done, deadlock)
  File "/home/runnphoenix/work/flatland-rl/rainbow/dueling_double_dqn.py", line 240, in _append_sample
    Q_expected = self.compute_Q_values([state], [mean_field], "local")
  File "/home/runnphoenix/work/flatland-rl/rainbow/dueling_double_dqn.py", line 373, in compute_Q_values
    network.train()
  File "/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1277, in train
    module.train(mode)
  File "/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1277, in train
    module.train(mode)
  File "/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1275, in train
    self.training = mode
  File "/home/runnphoenix/anaconda3/lib/python3.8/site-packages/torch/nn/modules/module.py", line 811, in __setattr__
    elif modules is not None and name in modules:
KeyboardInterrupt